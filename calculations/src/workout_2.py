# -*- coding: utf-8 -*-
"""workout.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o4Msj6lN_oJ-APQyjrVmEo5sawluugtM
"""

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import  KNeighborsRegressor
import numpy as np
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt

"""# understanding data"""

data = pd.read_csv("/content/gym_members_exercise_tracking.csv")

data.head()

data.info()

data["Workout_Type"].value_counts()

data["Workout_Type"].unique()

data.head()

import matplotlib.pyplot as plt

import seaborn as sns

for value in data["Gender"].unique():
  plt.figure(figsize=(12,6))

  sns.lineplot(x='Session_Duration (hours)',y='Water_Intake (liters)',data=data[data["Gender"] == value])

  plt.title(f'Liters of Water Intake by Duration workout {value}')

  plt.show()

for value in data["Workout_Type"].unique():
  plt.figure(figsize=(12,6))

  sns.lineplot(x='Session_Duration (hours)',y='Water_Intake (liters)',data=data[data["Workout_Type"] == value])

  plt.title(f'Liters of Water Intake by Duration Type workout {value}')

  plt.show()

def one_encode_workout_type(workout):
  data = {
      "Yoga": "0" ,
      "HIIT" : "1" ,
      "Cardio": "2",
      "Strength":"3"
  }
  return data.get(workout)

def one_encode_gender(gender):
  data = {
      "Female": "0" ,
      "Male" : "1" ,

  }
  return data.get(gender)

data["Workout_Type"] = data["Workout_Type"].apply(one_encode_workout_type)
data["Gender"] = data["Gender"].apply(one_encode_gender)

# correlation_matrix = data.corr()

# plt.figure(figsize=(10, 8))

# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

# plt.title('Correlation Matrix', fontsize=16)

# plt.show()

data.info

data.head()

data["Age"] = data["Age"].astype('object')
data["Experience_Level"] = data["Experience_Level"].astype('object')
data["Workout_Frequency (days/week)"] = data["Workout_Frequency (days/week)"].astype('object')

X = data.drop(columns = ["Calories_Burned"])
Y = data["Calories_Burned"]

data.describe()

data.describe(include=['O'])

plt.scatter(range(len(Y)), Y, color='blue')
plt.title('Calories_Burned')
plt.show()

plt.boxplot(Y, vert=False)
plt.title("Detecting outliers using Boxplot")
plt.xlabel('Sample')
plt.show()

def detect_outliers(data):
    # Menghitung mean dan standar deviasi data
    mean = np.mean(data)
    std_dev = np.std(data)

    # Menghitung Z-score untuk setiap data point
    z_scores = np.abs((data - mean) / std_dev)

    # Mendeteksi outlier berdasarkan rule Z-score
    outliers = np.where(z_scores > 3)[0]  # Indeks data yang merupakan outlier
    if len(outliers) > 0:
      return outliers
    else:
      return None

Y[detect_outliers(Y)]

"""Label are detected from outlier"""

feature_x = data[["Weight (kg)","Session_Duration (hours)","Fat_Percentage","Water_Intake (liters)"]]

for features in feature_x:
  if detect_outliers(data[features]):
    print(f"detected outlier: {detect_outliers(data[features])}")
  print(f"{features} -> not detected outlier")

"""Treatment using method standarization

"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
Y = Y.to_frame('calories_burned')
y_scaler = scaler.fit_transform(Y)

"""Feature clear not detected from outlier

# feature selection ( without fat percentage) Still maintence error when input 6 redundant with feature fat percentage
"""

data_without_fat_percentage = data.drop(columns=["Fat_Percentage"])
X_without_fat_percentage = X.drop(columns=["Fat_Percentage"])

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_selection import mutual_info_regression
import matplotlib.pyplot as plt
# %matplotlib inline

importances = mutual_info_regression(X_without_fat_percentage, y_scaler)
feat_importances = pd.Series(importances, data_without_fat_percentage.columns[0:len(data_without_fat_percentage.columns)-1])
feat_importances.plot(kind='barh', color = 'teal')
plt.show()

feat_df= feat_importances.to_frame().reset_index()
feat_df=feat_df.rename(columns= {0: 'value','index':"feature"})
feat_df

best_feature_col_without_fat_percentage = feat_df[feat_df["value"] >= 0.1]
best_feature_col_without_fat_percentage = best_feature_col_without_fat_percentage.feature

"""##model selection"""

X_without_fat_percentage = data[best_feature_col_without_fat_percentage ]
y = data['Calories_Burned']

"""##split test"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

def plot_graph(y_test,y_pred,regressorName):
    if max(y_test) >= max(y_pred):
        my_range = int(max(y_test))
    else:
        my_range = int(max(y_pred))

    plt.scatter(range(len(y_test)), y_test, color='blue',label='Data testing')
    plt.scatter(range(len(y_pred)), y_pred, color='red', label='Prediction')
    plt.title(regressorName)
    plt.legend()
    plt.show()
    return
def best_model_estimators(data_feature,
                         data_label,
                         model,
                         test_split = 0.5,
                         train_split=0.5,
                         grid_name = None):

  for test_size in np.arange(start=0.1, stop=0.6, step=0.1):
      train_size = 1 - test_size  # Ensure train + test <= 1

      # Split data
      X_train, X_test, y_train, y_test = train_test_split(
          data_feature, data_label, test_size=test_size, train_size=train_size
      )



      # Fit the model
      model.fit(X_train, y_train)

      # Get the best model and evaluate it
      if grid_name == True:
        best_model = model.get_params()[grid_name].best_estimator_
        print(f"\nFor test size {test_size} and train size {train_size}:")
        print(f"Best Parameters: {model.best_params_}")
        print(f"Best Cross-Validation Score: {model.best_score_}")

        # Test the model on the test set
        y_pred = best_model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)

        print(f"Test Set Metrics:")
        print(f"  R^2 Score: {r2:.4f}")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  Mean Absolute Error: {mae:.4f}")
      else:
        best_model = model.best_estimator_
        print(f"\nFor test size {test_size} and train size {train_size}:")
        print(f"Best Parameters: {model.best_params_}")
        print(f"Best Cross-Validation Score: {model.best_score_}")

        # Test the model on the test set
        y_pred = best_model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)

        print(f"Test Set Metrics:")
        print(f"  R^2 Score: {r2:.4f}")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  Mean Absolute Error: {mae:.4f}")
        plot_graph(y_test, y_pred, best_model)

X_without_fat_percentage_scaler = scaler.fit_transform( X_without_fat_percentage)
param_knn = {
    'n_neighbors':np.arange(stop = 20, step = 1, start = 1),
     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
     'metric': ['l1','l2']
}
clf_knn = GridSearchCV(KNeighborsRegressor(),
                       param_knn,
                       )
pipeline_knn_regressor =  Pipeline([('scaler', StandardScaler()), ('knn_regressor', clf_knn )])

best_model_estimators(
     X_without_fat_percentage_scaler,
     y_scaler,
     clf_knn
 )

param_dec_tree = {
    'criterion' : [ "squared_error", "friedman_mse", "absolute_error", "poisson"],
    'splitter' : ['best','random'],
    'max_depth' : np.arange(stop = 10 , step =1, start =  1),
    'min_samples_split':np.arange(stop = 10, step =1, start = 2)

}
clf_tree_without_fat_percentage = GridSearchCV(DecisionTreeRegressor(),
                       param_dec_tree,
                       return_train_score=False)

best_model_estimators(
     X_without_fat_percentage,
     y,
     clf_tree_without_fat_percentage
 )

"""Best model balance no overfitting and underfitting is
Decision tree with train split test, with best params:
  * test size 0.2 and train size 0.8:
  * Best Parameters: {'criterion': 'absolute_error', 'max_depth': 4, 'min_samples_split': 2, 'splitter': 'best'}
  * Best Cross-Validation Score: 0.8083690984203628
  * Test Set Metrics:
    * R^2 Score: 0.8109
    * Mean Squared Error: 14153.9679
    * Mean Absolute Error: 93.2436



"""

X_train, X_test, y_train, y_test = train_test_split(
          X_without_fat_percentage, y, test_size=0.2, train_size=0.8
      )

tree_regressor = DecisionTreeRegressor(
    criterion = 'absolute_error',
    max_depth = 4,
    min_samples_split = 2,
    splitter = 'best'
)

tree_regressor.fit(X_train, y_train)

pred_tree_regressor = tree_regressor.predict(X_test)

"""## Evaluation"""

def evaluation_regression(data_actual,
                         data_prediction,
                         model):
        r2 = r2_score(data_actual, data_prediction)
        mse = mean_squared_error(data_actual, data_prediction)
        mae = mean_absolute_error(data_actual, data_prediction)

        print(f"Test Set Metrics:")
        print(f"  R^2 Score: {r2:.4f}")
        print(f"  Mean Squared Error: {mse:.4f}")
        print(f"  Mean Absolute Error: {mae:.4f}")
        plot_graph(data_actual, data_prediction, model)

evaluation_regression(y_test, pred_tree_regressor, tree_regressor)

"""## Save model"""

import pickle
filename_simple_model = 'model_tree_simple.pkl'
pickle.dump(tree_regressor, open(filename_simple_model, 'wb'))

loaded_model = pickle.load(open(filename_simple_model, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result)

def encode_type_workout(workout):
  data = {
      0: "Yoga" ,
      1 : "HIIT" ,
      2: "Cardio",
      3:"Strength"
  }
  return data.get(workout)
def calculation_tree(
    Weight,
    Session_Duration,
    Workout_Type,
    Water_Intake,
    Workout_Frequency,
):
    try:

        # Prepare the input data
        input_data = np.array(
            [
                [
                    Weight,
                    Session_Duration,
                    Workout_Type,
                    Water_Intake,
                    Workout_Frequency,
                ]
            ]
        )

        # Make the prediction
        result  = loaded_model.predict(input_data)
        return {
            "status": "success",
            "input": {
                "Weight": Weight,
                "Session_Duration": Session_Duration,
                "Workout_Type": encode_type_workout(Workout_Type),
                "Water_Intake": Water_Intake,
                "Workout_Frequency": Workout_Frequency,
            },
            "hasil": result[0],  # Convert to list for JSON serialization
        }

    except FileNotFoundError as e:
        return {"status": "error", "message": str(e)}

    except Exception as e:
        return {"status": "error", "message": f"An error occurred: {str(e)}"}

calculation_tree(88.3, 1.69, 3, 3.5, 4)